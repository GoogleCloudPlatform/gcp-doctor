{% block confirmation_skipped_reason %}
All backends are healthy in backend service {name} in scope {region}
{% endblock confirmation_skipped_reason %}

{% block confirmation_failure_reason %}
 We found unhealthy backends in backend service {name} in scope {region}
 {detailed_reason}
{% endblock confirmation_failure_reason %}

{% block confirmation_failure_remediation %}
Further investigation is required to identify the root cause and apply appropriate fixes.
{% endblock confirmation_failure_remediation %}

{% block logging_enabled_success_reason %}
Health check logging is enabled
{% endblock logging_enabled_success_reason %}

{% block logging_enabled_uncertain_reason %}
Logging is not enabled for health check
{% endblock logging_enabled_uncertain_reason %}

{% block logging_enabled_uncertain_remediation %}
Enable logging for the health check using the following gcloud command:

gcloud compute health-checks update {protocol} {hc_name} {additional_flags}--enable-logging

This will log any future changes in health status but won't show any past activity. Note that the new health check logs will only appear when a health state transition occurs.
{% endblock logging_enabled_uncertain_remediation %}

{% block port_mismatch_uncertain_reason %}
The load balancer is performing health checks on port {hc_port}. We detected that within some backend instance groups, this is different than the port that the load balancer is using for serving traffic. The backend service is configured to use the "{serving_port_name}" port, which is translated to a port number based on the "{serving_port_name}" port mapping within each backend instance group.

Affected backends:
{formatted_igs}

This configuration can be problematic unless you have configured the load balancer to use a different port for health checks purposefully.
{% endblock port_mismatch_uncertain_reason %}

{% block port_mismatch_success_reason %}
The load balancer is performing health checks on the same port that it is using for serving traffic. This is the standard configuration.
{% endblock port_mismatch_success_reason %}

{% block firewall_rules_failure_reason %}
The health checks are currently failing due to a misconfigured firewall. This is preventing Google Cloud probers from connecting to your backends, causing the load balancer to consider them unhealthy.
{% endblock firewall_rules_failure_reason %}

{% block firewall_rules_failure_remediation %}
Update your firewall rules to allow inbound traffic from the Google Cloud health check IP ranges (found at https://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges) to your backends.
{% endblock firewall_rules_failure_remediation %}

{% block firewall_rules_success_reason %}
Firewalls are correctly configured and are not blocking the health check probes.
{% endblock firewall_rules_success_reason %}

{% block past_hc_success_uncertain_remediation %}
Check the logs and monitoring metrics for those instances, focusing on the given timeframes to see if there were any errors, crashes, or resource exhaustion issues that coincide with the unhealthy transition. You can also inspect any application-specific logs for errors or warnings around the timestamp.
{% endblock past_hc_success_uncertain_remediation %}

{% block unknown_hc_state_log_uncertain_reason %}
In the health check logs, we observed entries with the detailed health state "UNKNOWN." This indicates that the health checking system is aware of the instance, but its health status is currently undetermined. This situation can arise when a new endpoint is unresponsive to health checks and there's a substantial configured timeout period (approximately 25 seconds or longer). In such cases, the "UNKNOWN" state might be published while the health checker waits for the timeout to expire. Additionally, "UNKNOWN" could also be published during outage scenarios if the health checkers themselves are crashing. In this critical situation, endpoints that previously had known health states could transition to "UNKNOWN.
{% endblock unknown_hc_state_log_uncertain_reason %}

{% block unknown_hc_state_log_uncertain_remediation %}
For the new endpoints: Consider reducing the timeout period for health checks if appropriate, especially during initial setup or testing phases.
For potential Google Cloud outages: Use Personalized Service Health to check for any ongoing incidents that might be affecting your project or the specific service in question. If an incident is identified, follow any recommended mitigation steps or wait for the issue to be resolved by Google Cloud.
{% endblock unknown_hc_state_log_uncertain_remediation %}

{% block unhealthy_hc_state_log_uncertain_reason %}
In the health check logs, we found logs with the detailed health state UNHEALTHY, which means the endpoint is reachable but does not conform to the requirements defined by the health check.
The following responses were received from your backends: {probe_results_text_str}
{% endblock unhealthy_hc_state_log_uncertain_reason %}

{% block unhealthy_hc_state_log_uncertain_remediation %}
{success_criteria}

Please investigate the configuration of your application to ensure it aligns with these health check expectations.

If you intend to check a different endpoint or expect a different response, adjust the health check settings accordingly.
{% endblock unhealthy_hc_state_log_uncertain_remediation %}

{% block timeout_hc_state_log_uncertain_reason %}
In the health check logs, we found logs with the detailed health state TIMEOUT.
The backend might be timing out because:
1. The application is overloaded and taking too long to respond.
2. The backend service or health check timeout is too low.
3. Connection to the endpoint cannot be established - the backend instance has crashed or is otherwise unresponsive.
The following responses were received from your backends: {probe_results_text_str}
{% endblock timeout_hc_state_log_uncertain_reason %}

{% block timeout_hc_state_log_uncertain_remediation %}
1. Make sure that the backend service timeout (current value: {bs_timeout_sec}s) and health check timeout (current value: {hc_timeout_sec}s) are appropriately configured to accommodate your application's expected response time.
2. Investigate your application's configuration to ensure it is correctly handling health check probe requests. {success_criteria}
3. Check if firewall rules or iptables configurations are not blocking the health check probes from reaching the backend instances, resulting in timeouts.
{% endblock timeout_hc_state_log_uncertain_remediation %}
