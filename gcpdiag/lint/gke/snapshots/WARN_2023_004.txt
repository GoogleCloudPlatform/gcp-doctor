*  gke/WARN/2023_004: A Node Pool doesn't have too low `maxPodsPerNode` number
   - projects/gcpdiag-gke1-aaaa/locations/europe-west4/clusters/gke2/nodePools/default-pool [ OK ]
   - projects/gcpdiag-gke1-aaaa/locations/europe-west4/clusters/gke2/nodePools/low-pod-per-node-pool [FAIL]
     the nodepool has too low `maxPodsPerNode` number: 8
   - projects/gcpdiag-gke1-aaaa/locations/europe-west4/clusters/gke3/nodePools/default-pool [ OK ]
   - projects/gcpdiag-gke1-aaaa/zones/europe-west4-a/clusters/gke1/nodePools/default-pool [ OK ]
   - projects/gcpdiag-gke1-aaaa/zones/europe-west4-a/clusters/gke4/nodePools/default-pool [ OK ]
   - projects/gcpdiag-gke1-aaaa/zones/europe-west4-a/clusters/gke6/nodePools/default-pool [ OK ]

   Modern GKE clusters could run multiple system DaemonSets, and enabling a GKE
   feature could add another DaemonSet or two. 7+ DaemonSets is the norm for an
   average GKE cluster. Low `maxPodsPerNode` number could prevent normal
   workload scheduling as all the available slots could be occupied by system or
   custom DaemonSet pods. `maxPodsPerNode` >= 16 should be a safer option.

   https://gcpdiag.dev/rules/gke/WARN/2023_004

